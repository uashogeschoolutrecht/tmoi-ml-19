{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make some dummy data $X$. This is a matrix that contains $n$ observations, each observations having $k$ features, thus forming an $n \\times k$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[6 3 7]\n [4 6 9]\n [2 6 7]\n ...\n [2 0 6]\n [9 4 4]\n [0 8 6]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "n = 1000\n",
    "k = 3\n",
    "X = np.random.randint(10, size=(n, k))\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the first observation consists of three features. The first feature has a value of 6, the second of 3, the third of 7. To make predicication, we will want to learn what weight we should assign to every feature. Not every feature will be as important in predicting an outcome. This will depend on what we want to predict.\n",
    "\n",
    "However, at the beginning of each learning algorithm we will randomize the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.37454012],\n",
       "       [0.95071431],\n",
       "       [0.73199394]])"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "W = np.random.rand(k, 1)\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal will be to learn the proper weights for our type of prediction.\n",
    "\n",
    "E.g. If the features are length, weight and body temperature, the first two features will be roughly equally important in predicting the size of someones clothes, while the third will be of no importance. \n",
    "\n",
    "The final weights for this case might look something like $[0.9, 0.8, 0.01]$\n",
    "\n",
    "However, for predicting shoe size, the first feature will be more important than the last one. We might get something like $[0.9, 0.3, 0.01]$. \n",
    "\n",
    "For predicting infections, the last feature will be the most imporant, e.g. $[0.01, 0.1, 0.9]$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to make a prediction, often denoted as $\\hat{y}$ (y-hat), a very general way to do so is to multiply every feature with a weight:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y} = w_1 x_1 + \\dots + w_n x_n\n",
    "\\end{equation}\n",
    "\n",
    "This approach is a basic element in many machine learning algorithms. We might add some extra tricks, like Baysian Statistics, SVM's or stacking Neural Network layers upon eachother.\n",
    "\n",
    "A naive way to calculate this, would be while using a forloop. Making an empty matrix of zeros with the right shape is much faster then appending an outcome to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_loop(X, W, result):\n",
    "    # for every observation\n",
    "    for i, observation in enumerate(X):\n",
    "        yhat = 0\n",
    "        # we want to multiply every feature with a weight\n",
    "        for j, feature in enumerate(observation):\n",
    "            # and make a summation of every weigth * feature\n",
    "            yhat += W[j] * feature\n",
    "        result[i, 0] = yhat\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15 ms ± 550 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "result = np.zeros((X.shape[0],W.shape[1]))\n",
    "\n",
    "looptime = %timeit -o for_loop(X, W, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "457 ms ± 38.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "n = 10000\n",
    "k = 10\n",
    "X = np.random.randint(10, size=(n, k))\n",
    "W = np.random.rand(k, 1)\n",
    "result = np.zeros((X.shape[0],W.shape[1]))\n",
    "looptime = %timeit -o for_loop(X, W, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably, you will have a result somewhere in the millisecond range. With $n=1000$ and $k=3$, my laptop returns about 10 ms. Scaling this to n = 10000 and k = 10 gives about 300 ms. Even though this might seem fast, let's compare this to matrix multiplication.\n",
    "\n",
    "Note that, for matrix multiplication to work, we will need the dimensions of the two matrices to match. If our first matrix $X$ has dimensions $(n, k)$ and our second matrix $W$ has dimensions $(k, m)$, we are able to do $X \\dot W$ because the dimension $k$ is equal.\n",
    "\n",
    "So, trying to do a matrix multiplication with dimensions (10, 3) and (3, 1) will work, but (10,3) and (4,1) will fail because $3\\neq 4$.\n",
    "\n",
    "Also, trying to multiply (3,1) and (10, 3) will fail. This means the order is important. This is different from normal multiplication, where $2 \\times 3$ and $3 \\times 2$ both equal to 6.\n",
    "\n",
    "Let's say our data has $n=2$ cases ($a$ and $b$), where we observe $k=2$ features for every case. We now need two weights $w_1$ and $w_2$, one for every feature. A matrix multiplication would do exactly what we did with the forloop:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\n",
    "a_{1} & a_2 \\\\\n",
    "b_{1} & b_{2}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_1 \\\\\n",
    "w_2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_1*w_1 + a_2*w_2\\\\\n",
    "b_1*w_1 + b_2*w_2\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we implement the same formula, we see that everything gets much more compact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorisation(X, W):\n",
    "    return np.dot(X, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "105 µs ± 17.9 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "n = 10000\n",
    "k = 10\n",
    "X = np.random.randint(10, size=(n, k))\n",
    "W = np.random.rand(k, 1)\n",
    "vectortime = %timeit -o vectorisation(X, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to more compact code, we get a huge speed improvement.\n",
    "$n=10000$ and $k=10$ with matrix multiplication gives a result around 60 microseconds. That is more then a factor 1000 faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4336.634931970708"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "np.mean(looptime.timings) / np.mean(vectortime.timings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double check we got the same outcome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.6253665080512292e-12"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "result = np.zeros((X.shape[0],W.shape[1]))\n",
    "np.sum(for_loop(X, W, result) - vectorisation(X, W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while there is a very small difference, this is due to rounding errors and the **summed** error is smaller dan $10^{-12}$. This is due to rouding errors.\n",
    "\n",
    "This same principle works for a lot of numpy functions. Let's say we want to take the sinus of 100 values in a vector. Instead of going through a forloop like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "170 µs ± 10.2 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.rand(100)\n",
    "%timeit for x in X: np.sin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.89 µs ± 246 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.sin(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which gives us the exact same results.\n",
    "\n",
    "In addition to this, we don't have to worry too much about adding extra dimensions. Numpy simply scales things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[0.8239886 , 0.71848961, 0.06993476, 0.05604759, 0.04581963,\n",
       "         0.659005  , 0.49135754, 0.14744814, 0.80735118, 0.19946092],\n",
       "        [0.6607936 , 0.84065827, 0.83632242, 0.58049584, 0.15250417,\n",
       "         0.83756552, 0.6644321 , 0.73585227, 0.83124777, 0.2018063 ],\n",
       "        [0.31665034, 0.24345891, 0.83654922, 0.60873393, 0.30560616,\n",
       "         0.36193417, 0.43862713, 0.42468639, 0.1726946 , 0.75554553],\n",
       "        [0.69784989, 0.79172379, 0.50166134, 0.6225348 , 0.68454508,\n",
       "         0.5599242 , 0.6473771 , 0.509019  , 0.38890519, 0.8321013 ],\n",
       "        [0.45959484, 0.74324548, 0.56301806, 0.2690009 , 0.11266205,\n",
       "         0.79259332, 0.33093175, 0.29106101, 0.67959815, 0.60198278],\n",
       "        [0.61365912, 0.13135068, 0.24556778, 0.47576609, 0.76354648,\n",
       "         0.41268109, 0.14133345, 0.38970878, 0.28748325, 0.83853645],\n",
       "        [0.3670788 , 0.57743551, 0.54361226, 0.16926574, 0.12976418,\n",
       "         0.01766871, 0.13697929, 0.66438876, 0.59128406, 0.2802359 ],\n",
       "        [0.21356107, 0.03141019, 0.27933353, 0.33002384, 0.7611592 ,\n",
       "         0.01364659, 0.26087859, 0.81511626, 0.75254446, 0.77188435],\n",
       "        [0.00284869, 0.35438583, 0.46262995, 0.10753483, 0.08026472,\n",
       "         0.34971881, 0.79607598, 0.38982888, 0.68653896, 0.56303893],\n",
       "        [0.77544307, 0.32731587, 0.13272761, 0.1364414 , 0.18712319,\n",
       "         0.72340529, 0.54065934, 0.51898646, 0.12269856, 0.17673619]],\n",
       "\n",
       "       [[0.20383815, 0.68361618, 0.64081979, 0.55097343, 0.10155914,\n",
       "         0.15352353, 0.25445977, 0.73851661, 0.66111773, 0.79244081],\n",
       "        [0.03406477, 0.73199383, 0.02402075, 0.61742884, 0.60512223,\n",
       "         0.26473604, 0.68080322, 0.409846  , 0.59187297, 0.58933424],\n",
       "        [0.77170612, 0.62423693, 0.62148426, 0.56170325, 0.0999926 ,\n",
       "         0.2964846 , 0.44341753, 0.24771622, 0.16482229, 0.48965743],\n",
       "        [0.74141007, 0.59244052, 0.64013179, 0.14097463, 0.71563424,\n",
       "         0.11076931, 0.27831062, 0.768791  , 0.796689  , 0.08757482],\n",
       "        [0.1614112 , 0.3507934 , 0.656723  , 0.14852137, 0.15411595,\n",
       "         0.5188063 , 0.5564916 , 0.08783895, 0.3940942 , 0.66852801],\n",
       "        [0.33496497, 0.23984201, 0.11409398, 0.44811811, 0.80761754,\n",
       "         0.57618694, 0.06650156, 0.2555347 , 0.67481193, 0.68660181],\n",
       "        [0.36697264, 0.06691493, 0.51144417, 0.66876755, 0.32573861,\n",
       "         0.70941544, 0.32344789, 0.16807059, 0.71658913, 0.13326699],\n",
       "        [0.24185661, 0.7535178 , 0.49937823, 0.15360564, 0.57747264,\n",
       "         0.55298819, 0.52375844, 0.33400235, 0.39554934, 0.68959449],\n",
       "        [0.79938387, 0.54631534, 0.80307335, 0.17490838, 0.28601793,\n",
       "         0.74190605, 0.82103726, 0.77734151, 0.47187292, 0.72487274],\n",
       "        [0.19978118, 0.3119897 , 0.84069599, 0.1776819 , 0.24020993,\n",
       "         0.66151268, 0.62026627, 0.65792139, 0.81347873, 0.11948417]],\n",
       "\n",
       "       [[0.54622942, 0.25428108, 0.38413133, 0.3185988 , 0.80203945,\n",
       "         0.81129085, 0.47786519, 0.6659764 , 0.20988345, 0.33159447],\n",
       "        [0.61171253, 0.62292571, 0.788867  , 0.68489214, 0.74642594,\n",
       "         0.73344031, 0.26797077, 0.72679043, 0.20488042, 0.01260152],\n",
       "        [0.57759454, 0.35636597, 0.62099374, 0.2204727 , 0.2479146 ,\n",
       "         0.78304999, 0.61690406, 0.0891006 , 0.34322003, 0.60538901],\n",
       "        [0.65676055, 0.4768161 , 0.24929341, 0.62029414, 0.08382079,\n",
       "         0.71182276, 0.68589007, 0.28373604, 0.06070597, 0.0892035 ],\n",
       "        [0.4138054 , 0.18261175, 0.42276359, 0.05710984, 0.4286693 ,\n",
       "         0.78500613, 0.62745668, 0.0647715 , 0.08424495, 0.48616415],\n",
       "        [0.26061656, 0.42742354, 0.69475086, 0.38322113, 0.29601246,\n",
       "         0.78040633, 0.65453865, 0.5927197 , 0.07181825, 0.48019794],\n",
       "        [0.78005464, 0.73276599, 0.06433391, 0.68236625, 0.76678424,\n",
       "         0.2415549 , 0.49773201, 0.77429469, 0.77113557, 0.401159  ],\n",
       "        [0.48570244, 0.10598304, 0.50927714, 0.7585406 , 0.1719251 ,\n",
       "         0.44240932, 0.59978644, 0.04854954, 0.036547  , 0.75060039],\n",
       "        [0.8220813 , 0.68947452, 0.82101575, 0.15283028, 0.76444699,\n",
       "         0.78696634, 0.28608319, 0.60372885, 0.30110042, 0.10908587],\n",
       "        [0.72639577, 0.78701177, 0.10984888, 0.60374561, 0.82547237,\n",
       "         0.26973864, 0.68236612, 0.40071123, 0.06366505, 0.64399757]],\n",
       "\n",
       "       [[0.24344415, 0.56168198, 0.0706358 , 0.11993523, 0.00576859,\n",
       "         0.21612421, 0.13030793, 0.28946116, 0.20269842, 0.26055589],\n",
       "        [0.12836832, 0.17666243, 0.27826706, 0.42256755, 0.2597605 ,\n",
       "         0.18634445, 0.59441115, 0.35469011, 0.56741106, 0.38895629],\n",
       "        [0.40199756, 0.36735891, 0.75295379, 0.37438346, 0.56582105,\n",
       "         0.45004835, 0.55835554, 0.19453596, 0.11851037, 0.55682181],\n",
       "        [0.8322472 , 0.25505308, 0.00153313, 0.79125155, 0.13122109,\n",
       "         0.76252567, 0.49191689, 0.17786499, 0.19561372, 0.50309538],\n",
       "        [0.713623  , 0.72474143, 0.23578276, 0.65258958, 0.46142481,\n",
       "         0.58134207, 0.12929952, 0.42081969, 0.12294153, 0.481661  ],\n",
       "        [0.06141506, 0.13321212, 0.31942512, 0.4891908 , 0.11437287,\n",
       "         0.77013081, 0.27666541, 0.55121719, 0.14153405, 0.67327888],\n",
       "        [0.51385111, 0.80412302, 0.63438733, 0.83136955, 0.72596274,\n",
       "         0.50556367, 0.2231373 , 0.33387697, 0.3666925 , 0.79143378],\n",
       "        [0.25453019, 0.66101351, 0.55480538, 0.17099708, 0.26508879,\n",
       "         0.28015507, 0.17823762, 0.70646186, 0.30814671, 0.50878453],\n",
       "        [0.67192043, 0.82416183, 0.78960999, 0.21052161, 0.7796425 ,\n",
       "         0.74872332, 0.02991267, 0.77366643, 0.50936532, 0.19040644],\n",
       "        [0.83893504, 0.30685601, 0.18655662, 0.17214617, 0.50236161,\n",
       "         0.1738946 , 0.49672874, 0.67222974, 0.81262513, 0.35937613]],\n",
       "\n",
       "       [[0.82820785, 0.83542728, 0.55472119, 0.29788911, 0.75329511,\n",
       "         0.05322356, 0.42654036, 0.59219402, 0.83122862, 0.6406864 ],\n",
       "        [0.22908057, 0.75357305, 0.61131177, 0.76074409, 0.64105455,\n",
       "         0.34067975, 0.11631528, 0.63788791, 0.20177338, 0.06537907],\n",
       "        [0.28857996, 0.31172468, 0.83740452, 0.53410976, 0.83603025,\n",
       "         0.69788632, 0.4745605 , 0.28038102, 0.22231924, 0.23216481],\n",
       "        [0.75879437, 0.38608163, 0.43598791, 0.57874895, 0.05601502,\n",
       "         0.45711778, 0.01598513, 0.67786329, 0.02284627, 0.64809742],\n",
       "        [0.19947177, 0.7260371 , 0.31118119, 0.78612394, 0.67329418,\n",
       "         0.20846982, 0.77371153, 0.02045479, 0.58166592, 0.74549232],\n",
       "        [0.12914416, 0.04521719, 0.77630324, 0.29208932, 0.61723418,\n",
       "         0.74303635, 0.11006041, 0.36854873, 0.66456895, 0.07664886],\n",
       "        [0.68046364, 0.77113966, 0.71373862, 0.23680446, 0.74371682,\n",
       "         0.63661811, 0.67957718, 0.67985059, 0.43489607, 0.78400074],\n",
       "        [0.12359661, 0.81845473, 0.00671317, 0.69489385, 0.18505965,\n",
       "         0.06453738, 0.4787971 , 0.49146358, 0.61996589, 0.56011838],\n",
       "        [0.03124565, 0.20296713, 0.65637828, 0.80898416, 0.69332019,\n",
       "         0.83360978, 0.1239699 , 0.81830505, 0.3653581 , 0.5315271 ],\n",
       "        [0.82850694, 0.43275513, 0.30952055, 0.71088113, 0.64783137,\n",
       "         0.02183728, 0.4663341 , 0.79752706, 0.52021699, 0.5756953 ]],\n",
       "\n",
       "       [[0.46870261, 0.55300959, 0.29146382, 0.20185586, 0.38219916,\n",
       "         0.23021514, 0.78989558, 0.14805273, 0.23772878, 0.7098955 ],\n",
       "        [0.10184683, 0.71515924, 0.54658704, 0.4444441 , 0.76471114,\n",
       "         0.38820957, 0.27007814, 0.51671973, 0.52929022, 0.12253101],\n",
       "        [0.70705035, 0.36553278, 0.12574295, 0.09926788, 0.53557135,\n",
       "         0.36516172, 0.31203982, 0.44388796, 0.71198195, 0.16968297],\n",
       "        [0.11840939, 0.76492095, 0.72524356, 0.16281579, 0.52997219,\n",
       "         0.51732006, 0.69910681, 0.32329317, 0.06408357, 0.18580311],\n",
       "        [0.21038031, 0.44850585, 0.48250903, 0.20375257, 0.31340905,\n",
       "         0.70622402, 0.04332074, 0.67672673, 0.82925346, 0.69002517],\n",
       "        [0.80274246, 0.26021784, 0.49495327, 0.40284863, 0.26973519,\n",
       "         0.04057212, 0.68845281, 0.66277934, 0.53623215, 0.82710169],\n",
       "        [0.76248531, 0.53342912, 0.79593866, 0.53088485, 0.74300593,\n",
       "         0.48063842, 0.72179769, 0.43222299, 0.54093528, 0.19465518],\n",
       "        [0.57963158, 0.71592519, 0.74306382, 0.69541642, 0.01110938,\n",
       "         0.80313493, 0.05832083, 0.65125668, 0.73257549, 0.69403573],\n",
       "        [0.69751078, 0.22178602, 0.08673956, 0.45593461, 0.59051315,\n",
       "         0.73413254, 0.34523625, 0.73382771, 0.48080168, 0.70764214],\n",
       "        [0.22499333, 0.69255737, 0.21113449, 0.08091161, 0.7074694 ,\n",
       "         0.70417347, 0.10996926, 0.41289606, 0.03103921, 0.66087679]],\n",
       "\n",
       "       [[0.70251841, 0.62188903, 0.50271231, 0.13561627, 0.83462247,\n",
       "         0.25336272, 0.74370318, 0.07631468, 0.18972737, 0.63448022],\n",
       "        [0.04194874, 0.00715846, 0.57743597, 0.62404842, 0.65121553,\n",
       "         0.76041076, 0.31171407, 0.08659705, 0.38730312, 0.25157455],\n",
       "        [0.81822794, 0.19947573, 0.64413559, 0.79701754, 0.62052749,\n",
       "         0.06634551, 0.79929589, 0.14755274, 0.19845193, 0.6659867 ],\n",
       "        [0.47297027, 0.70186591, 0.16370202, 0.75565918, 0.29951186,\n",
       "         0.49804837, 0.11330392, 0.34390508, 0.35267439, 0.82310271],\n",
       "        [0.73861859, 0.05776352, 0.28178187, 0.20525527, 0.31352469,\n",
       "         0.01311483, 0.70477666, 0.13794483, 0.20134018, 0.61001583],\n",
       "        [0.5894953 , 0.73532549, 0.13665622, 0.21830863, 0.76832301,\n",
       "         0.61795221, 0.07886632, 0.09384695, 0.50847041, 0.68211357],\n",
       "        [0.59988054, 0.62721617, 0.54130664, 0.77160314, 0.78140552,\n",
       "         0.56923443, 0.66782957, 0.44614471, 0.26608823, 0.59418265],\n",
       "        [0.72781496, 0.25957303, 0.58584913, 0.58310929, 0.79488323,\n",
       "         0.68840196, 0.72518845, 0.54042184, 0.66515114, 0.24237413],\n",
       "        [0.54774456, 0.61493312, 0.74049176, 0.16494457, 0.32097318,\n",
       "         0.71136274, 0.42071681, 0.46571544, 0.58045395, 0.66478773],\n",
       "        [0.28430891, 0.76039415, 0.35618098, 0.25792326, 0.64387972,\n",
       "         0.35214555, 0.48499063, 0.32432658, 0.75357279, 0.47912702]],\n",
       "\n",
       "       [[0.06536963, 0.05667779, 0.35679125, 0.39640738, 0.03737472,\n",
       "         0.7528886 , 0.48519401, 0.52085578, 0.60662143, 0.21586979],\n",
       "        [0.49941993, 0.22101945, 0.40345514, 0.58658639, 0.23367885,\n",
       "         0.77301369, 0.28949375, 0.29909737, 0.79628941, 0.37944451],\n",
       "        [0.13611731, 0.41465222, 0.33258891, 0.4751521 , 0.57505502,\n",
       "         0.32373255, 0.65275649, 0.29817263, 0.74374663, 0.82039386],\n",
       "        [0.01367857, 0.11618256, 0.48822925, 0.58348354, 0.78986092,\n",
       "         0.58954636, 0.76063853, 0.49891534, 0.63625523, 0.79772516],\n",
       "        [0.62682493, 0.46413692, 0.55874972, 0.41713686, 0.14948776,\n",
       "         0.69624731, 0.40269005, 0.09463394, 0.64725569, 0.69184931],\n",
       "        [0.1050814 , 0.44024621, 0.46291519, 0.23236651, 0.7167787 ,\n",
       "         0.50618278, 0.5415892 , 0.35037838, 0.24304338, 0.71886793],\n",
       "        [0.50805162, 0.23856391, 0.41332515, 0.52761754, 0.52707689,\n",
       "         0.52926427, 0.16830257, 0.52682705, 0.33890073, 0.53717743],\n",
       "        [0.6182296 , 0.22843065, 0.12032406, 0.56826705, 0.63885928,\n",
       "         0.76169599, 0.24352793, 0.12362274, 0.82683674, 0.04000165],\n",
       "        [0.40426924, 0.47362137, 0.65239947, 0.72515944, 0.52916304,\n",
       "         0.02886523, 0.8404688 , 0.59702484, 0.73899564, 0.40279351],\n",
       "        [0.05220529, 0.08258403, 0.69692036, 0.21092884, 0.75543909,\n",
       "         0.2497098 , 0.25629403, 0.09770626, 0.13999215, 0.1268362 ]],\n",
       "\n",
       "       [[0.042163  , 0.23459943, 0.02248395, 0.64690155, 0.57798939,\n",
       "         0.17133642, 0.75925099, 0.28180098, 0.51042377, 0.72499162],\n",
       "        [0.36084229, 0.24841299, 0.75828037, 0.0739898 , 0.60553459,\n",
       "         0.74374954, 0.65197253, 0.3655705 , 0.81388855, 0.13543077],\n",
       "        [0.65299323, 0.65794064, 0.57478964, 0.46675703, 0.81475782,\n",
       "         0.44963176, 0.32696965, 0.49246095, 0.71800581, 0.16229321],\n",
       "        [0.36208549, 0.53597769, 0.74667435, 0.3897251 , 0.52310038,\n",
       "         0.12217888, 0.09397406, 0.21179101, 0.63316742, 0.21053453],\n",
       "        [0.76903724, 0.78254722, 0.26235801, 0.10507014, 0.39979457,\n",
       "         0.71295019, 0.10764101, 0.09907751, 0.24229808, 0.63861153],\n",
       "        [0.36565349, 0.67185445, 0.39383381, 0.07895507, 0.8406931 ,\n",
       "         0.62961351, 0.59016498, 0.47375153, 0.08303972, 0.83337539],\n",
       "        [0.4491086 , 0.04133478, 0.33655994, 0.03290761, 0.48536398,\n",
       "         0.6846324 , 0.81761295, 0.746901  , 0.26784207, 0.34011511],\n",
       "        [0.49665807, 0.6326894 , 0.67159067, 0.75099208, 0.22276745,\n",
       "         0.11148967, 0.17663597, 0.80955415, 0.13197877, 0.31355452],\n",
       "        [0.19452563, 0.11050033, 0.50273507, 0.37036325, 0.66433362,\n",
       "         0.38250231, 0.52746   , 0.73158458, 0.503373  , 0.42355934],\n",
       "        [0.13176681, 0.53066299, 0.09894128, 0.10469843, 0.43602982,\n",
       "         0.49635922, 0.67909442, 0.44083342, 0.58218634, 0.79548917]],\n",
       "\n",
       "       [[0.80756718, 0.26085827, 0.44048277, 0.50651402, 0.43223911,\n",
       "         0.82112781, 0.28626866, 0.4962239 , 0.12539192, 0.58902872],\n",
       "        [0.32231077, 0.72121432, 0.13000442, 0.77060409, 0.41695267,\n",
       "         0.39614863, 0.69139451, 0.297625  , 0.75617644, 0.27382312],\n",
       "        [0.66802222, 0.05583101, 0.64496491, 0.25962166, 0.66464195,\n",
       "         0.73580475, 0.16963059, 0.1662521 , 0.67801295, 0.16909448],\n",
       "        [0.01955417, 0.06159978, 0.02608846, 0.78793716, 0.76117431,\n",
       "         0.52728795, 0.49021086, 0.24146721, 0.47886034, 0.80237109],\n",
       "        [0.53980754, 0.056796  , 0.03699622, 0.76522021, 0.21527366,\n",
       "         0.00477809, 0.67706369, 0.73066044, 0.28240667, 0.73904142],\n",
       "        [0.72045813, 0.46779227, 0.80669112, 0.74774569, 0.02953956,\n",
       "         0.78318657, 0.51515073, 0.45171155, 0.81764519, 0.28096448],\n",
       "        [0.42492571, 0.18260077, 0.7655625 , 0.06763005, 0.01556314,\n",
       "         0.15119792, 0.59185102, 0.11719862, 0.79599725, 0.62759686],\n",
       "        [0.63103428, 0.82636072, 0.05739285, 0.60747847, 0.78277772,\n",
       "         0.57842318, 0.44413373, 0.02303056, 0.17037654, 0.79610435],\n",
       "        [0.8297218 , 0.47264372, 0.78239663, 0.46032267, 0.15850421,\n",
       "         0.08534468, 0.73983442, 0.80633311, 0.3983792 , 0.12654344],\n",
       "        [0.74920792, 0.33635666, 0.46818707, 0.83777824, 0.6272499 ,\n",
       "         0.66066235, 0.27761697, 0.25417706, 0.28953076, 0.56303983]]])"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "X = np.random.rand(100,100)\n",
    "np.log(X)\n",
    "np.exp(X)\n",
    "X = np.random.rand(10, 10, 10)\n",
    "np.sin(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variation on matrix multiplication is element wise multiplication, also known as the Hadamard product. Compare the two examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 7, 10],\n",
       "       [15, 22]])"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "A = np.array([[1,2],\n",
    "              [3,4]])\n",
    "\n",
    "B = np.array([[1,2],\n",
    "              [3,4]])\n",
    "np.dot(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we calculate $(1 \\times 1) + (2 \\times 3)$ for the first entry, which is 7.\n",
    "\n",
    "However, what if we wanted to multiply every element with the same element in the other matrix (so, every element at position $(i,j)$ in $A$ is multiplied with every $(i,j)$ element in $B$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 1,  4],\n",
       "       [ 9, 16]])"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "np.multiply(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This equals to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A * B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting referes to how numpy handles arrays with different shapes. In general, the smaller array is \"broadcast\" across the larger array to get compatible shapes. Multiplying 1-dimensional array of the same length performs a element-wise multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1., 4., 9.])"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "a = np.array([1.0, 2.0, 3.0])\n",
    "b = np.array([1.0, 2.0, 3.0])\n",
    "a * b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if the dimensions don't match, we get an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Error: operands could not be broadcast together with shapes (3,) (4,) \n"
     ]
    }
   ],
   "source": [
    "a = np.array([1.0, 2.0, 3.0])\n",
    "b = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "try:\n",
    "     a * b \n",
    "except Exception as e:\n",
    "    print('Error:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this constraint is relaxed when the dimensions meet certain constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([2., 4., 6.])"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "a = np.array([1.0, 2.0, 3.0])\n",
    "b = 2.0\n",
    "a * b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is equivalent to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([2., 4., 6.])"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "a = np.array([1.0, 2.0, 3.0])\n",
    "b = np.array([2.0, 2.0, 2.0])\n",
    "a * b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But just shorter. In the first case, we can think of `b` being strethced to an array of the same shape as `a`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, two dimensions are compatible when:\n",
    "1. they are equal\n",
    "2. one of them is 1\n",
    "\n",
    "In addition to this, array's do not need to have the same number of dimensions. Eg an image with dimensions of `256x256` pixels can have three colors. This results in a `256x256x3` shape matrix. If we want to scale each color, we can use a 1-dimensional array with 3 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(256, 256, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "image = np.random.rand(256, 256, 3)\n",
    "scale = np.array([2, 3, 6])\n",
    "output = image * scale\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([2, 3, 6])"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This \"broadcasting\" works along multiple dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(15, 3, 5)"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "A = np.random.rand(15, 3, 5)\n",
    "B = np.random.rand(15, 1, 5)\n",
    "C = A * B\n",
    "C.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or more complex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(8, 7, 6, 5)"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "A = np.random.rand(8, 1, 6, 1)\n",
    "B = np.random.rand(7, 1, 5)\n",
    "C = A*B\n",
    "C.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you figure out what happened here?"
   ]
  },
  {
   "source": [
    "# Exercise\n",
    "You receive data from 1000 patients. For every patient, there are 8 features."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1000, 8)"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "n = 1000\n",
    "k = 8\n",
    "X = np.random.rand(n, k)\n",
    "X.shape"
   ]
  },
  {
   "source": [
    "You want to build a simple linear model, and start by implementing the formula $f(x) = \\sum_{i=1}^n w_i * x_i $\n",
    "\n",
    "First, you initialize random weights. If you want to use matrix multiplication, what is the shape you need for the weights?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape van X: (1000,8)\n",
    "#shape W: (8,2)\n",
    "# shape np.dot(X, W)? (1000, 2)\n",
    "W = np.random.rand(8,1)"
   ]
  },
  {
   "source": [
    "If you multiply $X$ with $W$, what do you expect the shape of the outcome to be? In the outcome, what is the meaning of every row? And every column? Implement the forumula by using `np.dot`. Did you get the dimensions right?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "y = np.dot(X, W)\n",
    "y.shape"
   ]
  },
  {
   "source": [
    "Now, assume that instead of predicting one outcome, you want to predict two different things. Every patient still has 8 features, but the outcome should be two numbers, instead of one.\n",
    "\n",
    "How do you need to change the shape of $W$?\n",
    "\n",
    "What is the expected shape if you apply matrix multiplication between $X$ and $W$? What would happen if you multiply $W$ and $X$?\n",
    "\n",
    "Create $W$ and implement the new calculation. How does this differ from the calculation with just one outcome?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1000, 4)"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "W = np.random.rand(8,4)\n",
    "y = np.dot(X, W)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}