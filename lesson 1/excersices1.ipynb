{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.7 64-bit ('miniconda3': virtualenv)",
   "display_name": "Python 3.7.7 64-bit ('miniconda3': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "29ec8d00661af23ffb5ff5b75b393fc63c4adab68255a0d58b4504b328c061f2"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# ANCHORMEN ACADEMY\n",
    "# EXCERCISES 1 - SOLUTIONS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## SECTION A\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading libraries, some settings for plots\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [8, 8]\n",
    "sns.__version__ #should be >= 0.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = pd.read_csv('../data/cancer_data_uncleaned.csv', index_col=\"id\")"
   ]
  },
  {
   "source": [
    "Explore the data. Think of functions like `describe` or `info`. Take a good look at the columns, and clean up things that are obviously weird or wrong. Are there NaN's? Or exceptionally outliers, that are most likely a mistake? Scale the data to compare things in one plot."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Clean up the weird things, split into $X$ and $y$, scale the data.\n",
    "Check your result visually, eg with boxplots or a pairplot."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Split into a train and test set. Use a `test_size` of 0.3. \n",
    "\n",
    "If you use `random_state` 4, you will get comparable results with the solutions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection  import train_test_split\n",
    "X_train, X_test, y_train, y_test = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usefull for plotting the heatmap of the combination of the two parameters\n",
    "def gridsearch_heatmap(gridresults, param_grid, vmin = None, vmax = None):\n",
    "    # vmin and vmax change the contrast of the color scheme\n",
    "    idx, col = ['param_' + [*param_grid.keys()][i] for i in range(2)] \n",
    "    #idx, col = ['param_' + key for key in param_grid.keys()] \n",
    "    pivoted = pd.pivot_table(pd.DataFrame(gridresults.cv_results_),\n",
    "                            values = 'mean_test_score',\n",
    "                            index = idx,\n",
    "                            columns = col)\n",
    "    pivoted.columns = [\"{:.4f}\".format(x) for x in pivoted.columns]\n",
    "    #annot = pivoted.round(4)\n",
    "    sns.heatmap(pivoted, vmin = vmin, vmax = vmax, annot = True);"
   ]
  },
  {
   "source": [
    "Create a pipeline with a `StandardScaler` and a `SVC`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Build pipeline\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        # your code here\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "source": [
    "Create a parameter grid for the svm in the pipeline. Start with logarithmic steps for the values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Set parameter grid\n",
    "param_grid = {'svm__C': [ # your code here],\n",
    "              'svm__gamma': [ # your code here]}"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "Create a GridSearchCV with the pipe. Fit on the trainset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch = # your code here\n",
    "\n",
    "# Fit GridSearchCV\n",
    "gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "source": [
    "Visualize the results with the `gridsearch_heatmap` function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_heatmap(gridsearch, param_grid)"
   ]
  },
  {
   "source": [
    "What do you see? How do you explain this? Now, zoom in on the parameters. Take the area with the best heatspot and increase the granularity. Iterate until you are satisfied."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameter grid\n",
    "param_grid = # your code here\n",
    "# create pipe\n",
    "gridsearch = # your code here\n",
    "# fit the model\n",
    "gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase the imagesize, so you can read the numbers, and visualize again.\n",
    "# Set vmnin and vmax on the heatmap function for optimal contrast.\n",
    "plt.rcParams['figure.figsize'] = [15, 15]\n",
    "\n",
    "# your code here\n"
   ]
  },
  {
   "source": [
    "Print the best scores."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "source": [
    "Can you figure out how you can manually pick another value then the `best_score_`?\n",
    "Hint: look at the `set_params` method on the pipe. Fit and score the pipe with manually parameters, without running a gridsearch.\n",
    "\n",
    " What would you want to do that? At what point in the process is this a smart move? \n",
    " Based on what would you pick another value?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "source": [
    "Can you figure out how to add another kernel to the pipe? Read the documentation from sklearn on SVC to find out your options."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = # your code here\n",
    "\n",
    "gridsearch = # your code here\n",
    "\n",
    "# Fit GridSearchCV\n",
    "gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the outcomes.\n",
    "# print the best outcomes.\n",
    "# iterate as long as you think it can improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Can you figure out how to switch the SVC for a `RandomForestClassifier` in the pipe? Read the documentation from `sklearn`, and do a gridsearch on different numbers of `n_estimators`. Take a range you suspect to be interesting based on the documentation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Build pipeline\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        # your code here\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid = # your code here\n",
    "\n",
    "gridsearch = GridSearchCV(pipe, param_grid=param_grid, cv=3)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print the best outcomes\n",
    "# your code here"
   ]
  },
  {
   "source": [
    "How does this compare to the SVC? Can you plot the result for the different values of `n_estimators`? How does this compare to the `SVC`? How certain are you of the results on onseen data, based on these results? Which one would you pick, just based on these results?\n",
    "\n",
    "Hint: look at the `.cv_results_` object in the gridsearch. Decide how to plot this so you can visualize what is going on."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ]
}